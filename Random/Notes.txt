--------------------- Week 1
Functions used in PD - purpose :

Value.counts() used in catagorical values 
count() used in numrical values
df.groupby(['catagorical_column'])['Numircal column].sum() or count() etc .., or doing .agg ( New_Name_column = ['column' , 'sum'] ,  New_Name_column = ['column' , 'count'] .. etc) -- anytype of aggregations will work


---

Algorithms used ? :

one-hot encoding , etc.. encoding :
Used to prepare the data to train the ML Model - from catagorical values to boolean - binary " 0 , 1 "


---------------------- Week 3
polynomial ?
Why Create a pipeline for polynomial ?
= To check the best degree for the dataset

---------------
(estimator=rf, param_grid=hyperparameters, cv=5,  n_jobs=-1, verbose=1)
Jobs = the usage of CPU/GPU -1 = full usage

----------------------
MNIST Dataset: ( Week 2 - Day 4)
The MNIST dataset is a large database of handwritten digits commonly used for training various image processing systems.
It contains 70,000 images of handwritten digits from 0 to 9, making it a staple dataset for benchmarking classification algorithms.

--

We use segmoid when we are dealing with catagorical values and change it to 0 - 1 so we can deal with and predict it 
used sigmoid after find the result of 
Linear regression ( h(0) = theta0, theta1 x1 )
and putting this result in the segmoid function (Logistic regression)
More details : SDAIA-T5-Data-Science-and-AI-Bootcamp\Week2\2 - Foundations of Machine Learning\4- Supervised Learning II - Classification\LAB\Logistic_Regression.ipynb

---------------------- Week 2 day 4
Impalanced data ? -- need to read

--

softmax_reg.predict_proba([[5, 2]]) >> Print the propability of all classes in the model ( Higher value = assigned class)
softmax_reg.predict([[5, 2]]) >> print only the assigned class

----------------
We can use Decision Tree in both ( classification and regression )

-- 
RandomForest = More than 1 decision tree -
The input enter each tree and make a decision depending on all the trees - More decision from the trees occur = The result

-- 
We using This calculations in regression ONLY 

# Measure model performance
mse = mean_squared_error(y_train, tree_pred)
print("Mean Squared Error:", mse)

# Measure model performance
mae = mean_absolute_error(y_train, tree_pred)
print("Mean Absolute Error:", mae)

# Measure model performance
rmse = np.sqrt(mean_squared_error(y_train, tree_pred))
print("Root Mean Squared Error:", rmse)

---
In Classification We use F1-Score - Accurace - Precesion call

--- Week 3 - Bagging
Bagging > In Parallel
Bagging - With replacement
Bagging steps :
1- Random subset ( Booststrap = subset of the original sample - if with Booststrap = false thats mean without replacement)
2- Build the model
3- aggregations

--
Roughly Balanced Bagging (RBB) - Handling Impalanced dataset > the strength of this model

-------- Week 3 - Boosting
Bosting > Sequential 
Dataset > model > increase weight for miss prediction to avoid it in the next model
 > model > increase weight for miss value > etc ..

 -----
 MetaLarnar > Predict the best value in Logistic Regression From ( Stacking models )

----
DropOut acting same the browning on trees 
DropOut it's cutting the neural networks

----
Cross validation : To avoid overfitting
----
Augomintation > Increase the dataset with rotate, adding noise, etc .. to the images
---
Hetroginues model -- Can take different types of models
--
Need to read more about Boosting - Bagging - Stacking and check every algorithm why used ? adv / disadv ?

----------------------------------
Task-Driven                                 |              Data-Driven 
prediction depending on the labeled column  | Unsupervised - Without label (y)
(Supervised) with knowing the predicted
value result

----------------------------------
Association in Clusterting :
Find the relationsip between features - Association Rules :Association rules are used to find relationships between variables in 
large datasets. This technique is commonly used in market basket analysis to identify sets of 
products frequently bought together. 

-----------------------------------

Dropout in deep Learning means to drop a hidden layer randomly ( Like browning in DecisionTree )
----
Weight in neural networks = the strength of the connection
-
Perceptrons?
â€¢ Perceptrons are binary classification algorithms
-----
BackPropagation depend on chain rule way