{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "274661b6",
   "metadata": {},
   "source": [
    "# Web Scraping Tutorial\n",
    "\n",
    "This notebook provides a step-by-step guide to scrape data from a website. Web scraping is a technique used to extract information from websites by transforming the data on web pages into a structured format. This is particularly useful for data analysis, machine learning, and other data-driven tasks.\n",
    "\n",
    "In this tutorial, we will walk through the process of scraping product information from a sample e-commerce site. By following these steps, you will learn how to:\n",
    "\n",
    "1. Send HTTP requests to retrieve web pages.\n",
    "2. Parse HTML content using BeautifulSoup.\n",
    "3. Identify and extract relevant data elements from the parsed HTML.\n",
    "4. Store the extracted data in a structured format using pandas.\n",
    "5. Save the data to a CSV file.\n",
    "6. Optionally, save the data to a database such as MongoDB.\n",
    "\n",
    "The website we will be scraping is [ScrapeMe](https://scrapeme.live/shop/). This site is designed for practice purposes and contains a variety of products with details such as names and prices, which makes it an ideal candidate for learning web scraping techniques.\n",
    "\n",
    "Before you begin, please visit the site to understand its structure. This will help you identify the elements you need to scrape.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18a87e75",
   "metadata": {},
   "source": [
    "## Import libraries here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "5bdedab8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7f9f606",
   "metadata": {},
   "source": [
    "## Step 1: Send a request to the website"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "7de25bdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "8f4e3051",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "200"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'https://tuwaiq.edu.sa/TestCenter'\n",
    "\n",
    "response = requests.get(url)\n",
    "\n",
    "response.status_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71d1d25f",
   "metadata": {},
   "source": [
    "## Step 2: Parse the HTML content of the page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "e2630791",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<title>أكاديمية طويق</title>"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.text,'html.parser')\n",
    "\n",
    "soup.title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "136215c4",
   "metadata": {},
   "source": [
    "## Step 3: Inspect the website and identify the elements to scrape\n",
    "Inspect the website and identify the elements (e.g., product names, prices, etc.)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "162008b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[<h1 class=\"text-[#1E1E1E] font-bold text-3xl\">\n",
      "                    +300 شهادة احترافية عالمية في مجالات:\n",
      "                </h1>]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<ul class=\"grid items-start justify-between grid-cols-2 gap-4 md:flex\">\n",
       " <li class=\"flex flex-col items-center w-full md:-ms-10\">\n",
       " <span class=\"w-10 pt-5 border-t-8 border-primary-400\"></span>\n",
       "                             الذكاء الاصطناعي\n",
       "                         </li>\n",
       " <li class=\"flex flex-col items-center w-full\">\n",
       " <span class=\"w-10 pt-5 border-t-8 border-primary-400\"></span>\n",
       "                             تحليل وإدارة البيانات\n",
       "                         </li>\n",
       " <li class=\"flex flex-col items-center w-full\">\n",
       " <span class=\"w-10 pt-5 border-t-8 border-primary-400\"></span>\n",
       "                             الأمن السيبراني\n",
       "                         </li>\n",
       " <li class=\"flex flex-col items-center w-full\">\n",
       " <span class=\"w-10 pt-5 border-t-8 border-primary-400\"></span>\n",
       "                             الحوسبة السحابية\n",
       "                         </li>\n",
       " <li class=\"flex flex-col items-center w-full\">\n",
       " <span class=\"w-10 pt-5 border-t-8 border-primary-400\"></span>\n",
       "                             تقنية المعلومات\n",
       "                         </li>\n",
       " <li class=\"flex flex-col items-center w-full\">\n",
       " <span class=\"w-10 pt-5 border-t-8 border-primary-400\"></span>\n",
       "                             إدارة المشاريع\n",
       "                         </li>\n",
       " </ul>]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "print(soup.find_all('h1',class_= 'text-[#1E1E1E] font-bold text-3xl'))\n",
    "soup.find_all('ul',class_='grid items-start justify-between grid-cols-2 gap-4 md:flex')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64e51da7",
   "metadata": {},
   "source": [
    "## Step 4: Extract the desired data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "06b9477d",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=[]\n",
    "for li in soup.find_all('ul',class_='grid items-start justify-between grid-cols-2 gap-4 md:flex'):\n",
    "    arr = li.text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebe02982",
   "metadata": {},
   "source": [
    "## Step 5: Create a DataFrame to store the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0dfe22",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a7fb0de6",
   "metadata": {},
   "source": [
    "## Step 6: Save the data to a CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "24e44e64",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'requests' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [1]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m7\u001b[39m):\n\u001b[0;32m      2\u001b[0m     url \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://binbaz.org.sa/fatwas/kind/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m----> 3\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mrequests\u001b[49m\u001b[38;5;241m.\u001b[39mget(url)\n\u001b[0;32m      4\u001b[0m     soup \u001b[38;5;241m=\u001b[39m BeautifulSoup(response\u001b[38;5;241m.\u001b[39mtext, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhtml.parser\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      6\u001b[0m     question \u001b[38;5;241m=\u001b[39m soup\u001b[38;5;241m.\u001b[39mfind_all(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mh1\u001b[39m\u001b[38;5;124m'\u001b[39m)  \n",
      "\u001b[1;31mNameError\u001b[0m: name 'requests' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(1, 7):\n",
    "    url = f'https://binbaz.org.sa/fatwas/kind/{i}'\n",
    "    response = requests.get(url)\n",
    "    soup = BeautifulSoup(response.text, 'html.parser')\n",
    "\n",
    "    question = soup.find_all('h1')  \n",
    "    Answers = soup.find_all('p')\n",
    "q = {}\n",
    "for question in question:\n",
    "    print(\"Qustion : \" ,question.get_text())\n",
    "    q.insert(question.get_text())\n",
    "\n",
    "q"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee480802",
   "metadata": {},
   "source": [
    "## Step 7: Print the DataFrame to verify the extracted data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bee5d07c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ea5dc916",
   "metadata": {},
   "source": [
    "## Step 8: Save the data to a database of your choice. If you are using MongoDB, include the code here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec9b8a29",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
